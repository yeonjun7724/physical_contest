import os
import io
import base64
from dataclasses import dataclass
from typing import List, Optional, Literal

import cv2
import numpy as np
from PIL import Image

import streamlit as st
from openai import OpenAI


# ========= 1. ë°ì´í„° êµ¬ì¡° =========

@dataclass
class VLMAnalysisResult:
    test_type: str
    reps: int
    duration_sec: float
    depth_quality: Literal["poor", "fair", "good"]
    knee_alignment: Literal["valgus", "varus", "neutral"]
    tempo: Literal["slow", "steady", "fast"]
    stability: Literal["low", "medium", "high"]
    risk_flags: List[str]


@dataclass
class ScoringResult:
    is_valid_for_kfta: bool
    total_score: int
    grade: int
    detail: str


# ========= 2. í”„ë ˆì„ ì¶”ì¶œ =========

def extract_keyframes(video_path: str, num_frames: int = 8,
                      resize_to=(640, 360)) -> List[np.ndarray]:
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    idxs = np.linspace(0, frame_count - 1, num_frames, dtype=int)

    frames = []
    for idx in idxs:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = cv2.resize(frame, resize_to)
            frames.append(frame)
    cap.release()
    return frames


# ========= 3. mp4 ì˜ìƒ â†’ VLM ë¶„ì„ =========

def analyze_video_with_vlm(video_bytes, duration_sec, model="gpt-4.1"):
    api_key = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key=api_key)

    # mp4 â†’ base64
    b64 = base64.b64encode(video_bytes).decode()
    video_url = f"data:video/mp4;base64,{b64}"

    system_prompt = """
ë‹¹ì‹ ì€ ìš´ë™ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ìƒì„ ë³´ê³  ìš´ë™ ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    user_prompt = f"ì˜ìƒ ê¸¸ì´ëŠ” {duration_sec:.1f}ì´ˆì…ë‹ˆë‹¤. JSONìœ¼ë¡œ ë¶„ì„í•´ ì£¼ì„¸ìš”."

    resp = client.chat.completions.create(
        model=model,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt},
                    {"type": "input_video", "video_url": video_url},
                ],
            },
        ]
    )

    import json
    data = json.loads(resp.choices[0].message.content)

    return VLMAnalysisResult(
        test_type=data.get("exercise_type", "squat"),
        reps=int(data.get("estimated_reps", 0)),
        duration_sec=duration_sec,
        depth_quality=data.get("movement_quality", {}).get("depth", "fair"),
        knee_alignment=data.get("movement_quality", {}).get("knee_alignment", "neutral"),
        tempo=data.get("tempo", "steady"),
        stability=data.get("stability", "medium"),
        risk_flags=data.get("risk_factors", []),
    )


# ========= 4. ì ìˆ˜í™” =========

def score_against_kfta(a: VLMAnalysisResult, age_group, gender):
    max_reps = 30
    reps_score = min(a.reps / max_reps * 60, 60)

    depth_map = {"poor": 10, "fair": 20, "good": 30}
    depth_score = depth_map.get(a.depth_quality, 0)

    tempo_map = {"slow": 5, "steady": 10, "fast": 5}
    tempo_score = tempo_map.get(a.tempo, 0)

    stability_map = {"low": 0, "medium": 5, "high": 10}
    stability_score = stability_map.get(a.stability, 0)

    knee_penalty = 10 if a.knee_alignment == "valgus" else 5 if a.knee_alignment == "varus" else 0

    posture_score = depth_score + stability_score + tempo_score - knee_penalty
    posture_score = max(min(posture_score, 40), 0)

    total = int(min(reps_score + posture_score, 100))

    grade = 1 if total >= 90 else 2 if total >= 75 else 3 if total >= 60 else 4 if total >= 45 else 5

    is_valid = a.reps >= 5
    detail = "ì˜ˆë¹„ì¸¡ì • ê°€ëŠ¥" if is_valid else "ë°˜ë³µ ìˆ˜ ë¶€ì¡±"

    return ScoringResult(is_valid, total, grade, detail)


# ========= 5. ë¦¬í¬íŠ¸ =========

def generate_report_with_llm(analysis, score, model="gpt-4.1-mini"):
    api_key = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key=api_key)

    system_prompt = "ìš´ë™ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”."

    user_prompt = f"""
ë°˜ë³µìˆ˜: {analysis.reps}
ìŠ¤ì¿¼íŠ¸ ê¹Šì´: {analysis.depth_quality}
ë¬´ë¦ ì •ë ¬: {analysis.knee_alignment}
í…œí¬: {analysis.tempo}
ì•ˆì •ì„±: {analysis.stability}
ì´ì : {score.total_score}
"""

    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    return resp.choices[0].message.content


# ========= 6. Streamlit UI (ìƒˆ UX/UI) =========

def main():

    st.set_page_config(page_title="êµ­ë¯¼ì²´ë ¥100 AI ì˜ìƒ ë¶„ì„", layout="centered")

    st.title("AI ê¸°ë°˜ êµ­ë¯¼ì²´ë ¥100 ì˜ìƒ ë¶„ì„ (mp4 ì—…ë¡œë“œ ì „ìš©)")
    st.write("ì—…ë¡œë“œí•œ ìš´ë™ ì˜ìƒì„ ê¸°ë°˜ìœ¼ë¡œ VLMì´ ìë™ìœ¼ë¡œ ìŠ¤ì¿¼íŠ¸ ìì„¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

    # ğŸ”¥ğŸ”¥ğŸ”¥ ìœ íŠœë¸Œ ë§í¬ ì œê±° â†’ ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸ ì¶”ê°€ ğŸ”¥ğŸ”¥ğŸ”¥
    uploaded = st.file_uploader("ìš´ë™ ì˜ìƒ ì—…ë¡œë“œ (mp4)", type=["mp4"])

    # ì—…ë¡œë“œí•œ ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°
    if uploaded:
        st.video(uploaded)

    col1, col2 = st.columns(2)
    age_group = col1.selectbox("ì—°ë ¹ëŒ€", ["ì„ íƒ ì•ˆ í•¨", "10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€", "60ëŒ€ ì´ìƒ"])
    gender = col2.selectbox("ì„±ë³„", ["ì„ íƒ ì•ˆ í•¨", "ë‚¨ì„±", "ì—¬ì„±"])

    if st.button("ë¶„ì„ ì‹¤í–‰", type="primary"):

        if uploaded is None:
            st.error("mp4 íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return

        video_bytes = uploaded.read()

        # ---- ì˜ìƒ ê¸¸ì´ ----
        video_path = "uploaded_video.mp4"
        with open(video_path, "wb") as f:
            f.write(video_bytes)

        cap = cv2.VideoCapture(video_path)
        fps = max(cap.get(cv2.CAP_PROP_FPS), 1e-6)
        duration_sec = cap.get(cv2.CAP_PROP_FRAME_COUNT) / fps
        cap.release()

        # ---- ëŒ€í‘œ í”„ë ˆì„ ì¶”ì¶œ ----
        with st.spinner("ëŒ€í‘œ í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."):
            frames_np = extract_keyframes(video_path)

        # ---- mp4 ì˜ìƒ ì§ì ‘ ë¶„ì„ ----
        with st.spinner("VLMì´ ì˜ìƒì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
            analysis = analyze_video_with_vlm(video_bytes, duration_sec)

        # ---- ì ìˆ˜í™” ----
        score = score_against_kfta(analysis,
                                   None if age_group == "ì„ íƒ ì•ˆ í•¨" else age_group,
                                   None if gender == "ì„ íƒ ì•ˆ í•¨" else gender)

        # ---- ë¦¬í¬íŠ¸ ìƒì„± ----
        with st.spinner("AI ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
            report = generate_report_with_llm(analysis, score)

        # ê²°ê³¼ ì¶œë ¥
        st.subheader("1. ëŒ€í‘œ í”„ë ˆì„")
        st.image(frames_np, caption=[f"Frame {i+1}" for i in range(len(frames_np))], use_column_width=True)

        st.subheader("2. VLM ë¶„ì„ ê²°ê³¼(JSON)")
        st.json(analysis.__dict__)

        st.subheader("3. ì ìˆ˜ ê²°ê³¼")
        st.metric("ì´ì ", f"{score.total_score} / 100")
        st.metric("ì˜ˆìƒ ë“±ê¸‰", f"{score.grade} ë“±ê¸‰")

        st.subheader("4. AI ì½”ì¹˜ ë¦¬í¬íŠ¸")
        st.markdown(report)


if __name__ == "__main__":
    main()
